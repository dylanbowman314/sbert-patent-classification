{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dcbowma2/cs240/env/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, InputExample, losses\n",
    "from transformers import AutoModel\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "patent_data = pd.read_csv('df_claim_cpc_1400.csv',encoding='unicode_escape')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sort the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-processing to get the possible labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_level= {}\n",
    "mid_level = {}\n",
    "group_level = {}\n",
    "\n",
    "i=0\n",
    "for label in patent_data.iterrows():\n",
    "    tl = label[1]['group_id'][2]\n",
    "    ml = label[1]['group_id'][2:5]\n",
    "    gl = label[1]['group_id'][2:6]\n",
    "    sentence = label[1]['text'].encode().decode(\"utf-8\")\n",
    "    \n",
    "    if tl in top_level:\n",
    "        top_level[tl].append(sentence)\n",
    "    else:\n",
    "        top_level[tl] = [sentence]\n",
    "    \n",
    "    if ml in mid_level:\n",
    "        mid_level[ml].append(sentence)\n",
    "    else:\n",
    "        mid_level[ml] = [sentence]\n",
    "\n",
    "    if gl in group_level:\n",
    "        group_level[gl].append(sentence)\n",
    "    else:\n",
    "        group_level[gl] = [sentence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_labels(classes):\n",
    "    labels = {}\n",
    "    for i,l in enumerate(classes):\n",
    "        labels[l] = float(i)\n",
    "    return labels\n",
    "\n",
    "top_labels = gen_labels(top_level)\n",
    "mid_labels = gen_labels(mid_level)\n",
    "group_labels = gen_labels(group_level)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tune SBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sbert_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "sentences = patent_data['text']\n",
    "groups = patent_data['group_id']\n",
    "\n",
    "train_examples = []\n",
    "\n",
    "for t in group_level: \n",
    "    for k in group_level[t]:\n",
    "        train_examples.append(InputExample(texts=k, label=group_labels[t])) # can fine-tune on other label types\n",
    "\n",
    "#Define train dataset, the dataloader and the train loss\n",
    "train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=1)\n",
    "train_loss = losses.CosineSimilarityLoss(sbert_model)\n",
    "\n",
    "do_train = False\n",
    "\n",
    "if do_train:\n",
    "    sbert_model.fit(train_objectives=[(train_dataloader, train_loss)], epochs=2)\n",
    "    sbert_model.save(\"sbert\")\n",
    "else:\n",
    "    sbert_model = SentenceTransformer('sbert')\n",
    "\n",
    "embeddings = sbert_model.encode(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define generic softmax classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, output_size, input_size=384, hidden_size1=1000, hidden_size2=1000):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(input_size, hidden_size1)\n",
    "        self.linear2 = nn.Linear(hidden_size1, hidden_size2)\n",
    "        self.linear3 = nn.Linear(hidden_size2, output_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.linear3(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "    \n",
    "    def train(self, train_loader, criterion, optimizer, num_epochs):\n",
    "        for epoch in range(num_epochs):\n",
    "            for inputs, labels in train_loader:\n",
    "                # Clear the gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Compute the predicted outputs\n",
    "                outputs = self(inputs)\n",
    "\n",
    "                # Compute the loss\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                # Backpropagate the gradients\n",
    "                loss.backward()\n",
    "                \n",
    "                # Update the model parameters\n",
    "                optimizer.step()\n",
    "            if epoch % 100 == 0:\n",
    "                print(f\"Loss: {loss}, epoch: {epoch}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test-train split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_top = []\n",
    "test_data_top = []\n",
    "embeddings = torch.tensor(embeddings)\n",
    "for i in range(len(embeddings)):\n",
    "    if i < 1000:\n",
    "        train_data_top.append([embeddings[i], int(top_labels[patent_data['group_id'][i][2]])])\n",
    "    else:\n",
    "        test_data_top.append([embeddings[i], int(top_labels[patent_data['group_id'][i][2]])])\n",
    "\n",
    "train_data_mid = []\n",
    "test_data_mid = []\n",
    "\n",
    "for i in range(len(embeddings)):\n",
    "    if i < 1000:\n",
    "        train_data_mid.append([embeddings[i], int(mid_labels[patent_data['group_id'][i][2:5]])])\n",
    "    else:\n",
    "        test_data_mid.append([embeddings[i], int(mid_labels[patent_data['group_id'][i][2:5]])])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.195930004119873, epoch: 0\n",
      "Loss: 1.8704313039779663, epoch: 100\n",
      "Loss: 1.7475128173828125, epoch: 200\n",
      "Loss: 1.6224596500396729, epoch: 300\n",
      "Loss: 1.373646855354309, epoch: 400\n",
      "Loss: 1.618364930152893, epoch: 500\n",
      "Loss: 1.8566889762878418, epoch: 600\n",
      "Loss: 1.6169908046722412, epoch: 700\n",
      "Loss: 1.4975194931030273, epoch: 800\n",
      "Loss: 1.6158488988876343, epoch: 900\n"
     ]
    }
   ],
   "source": [
    "top_classifier = Classifier(output_size=len(top_level))\n",
    "\n",
    "top_classifier.train(\n",
    "    train_loader=DataLoader(train_data_top, shuffle=True, batch_size=16),\n",
    "    criterion=nn.CrossEntropyLoss(),\n",
    "    optimizer=torch.optim.SGD(top_classifier.parameters(), lr=0.01, momentum=0.9),\n",
    "    num_epochs=1000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 4.663356781005859, epoch: 0\n",
      "Loss: 4.6631760597229, epoch: 100\n",
      "Loss: 4.662642002105713, epoch: 200\n",
      "Loss: 4.66279935836792, epoch: 300\n",
      "Loss: 4.670515537261963, epoch: 400\n",
      "Loss: 4.430095672607422, epoch: 500\n",
      "Loss: 4.67944860458374, epoch: 600\n",
      "Loss: 4.42968225479126, epoch: 700\n",
      "Loss: 4.55458402633667, epoch: 800\n",
      "Loss: 4.429601669311523, epoch: 900\n"
     ]
    }
   ],
   "source": [
    "mid_classifier = Classifier(output_size=len(mid_level))\n",
    "\n",
    "mid_classifier.train(\n",
    "    train_loader=DataLoader(train_data_mid, shuffle=True, batch_size=16),\n",
    "    criterion=nn.CrossEntropyLoss(),\n",
    "    optimizer=torch.optim.SGD(mid_classifier.parameters(), lr=0.02),\n",
    "    num_epochs=1000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top_classifier train accuracy: 0.729\n",
      "top_classifier test accuracy: 0.44038929440389296\n",
      "mid_classifier train accuracy: 0.098\n",
      "mid_classifier test accuracy: 0.09732360097323602\n"
     ]
    }
   ],
   "source": [
    "def accuracy(model, data):\n",
    "    accurate = 0\n",
    "    for e,l in data:\n",
    "        sm = model.forward(torch.unsqueeze(e,0))\n",
    "        if int(torch.argmax(sm)) == l:\n",
    "            accurate += 1\n",
    "    return accurate/len(data)\n",
    "\n",
    "\n",
    "print(f\"top_classifier train accuracy: {accuracy(top_classifier, train_data_top)}\")\n",
    "print(f\"top_classifier test accuracy: {accuracy(top_classifier, test_data_top)}\")\n",
    "print(f\"mid_classifier train accuracy: {accuracy(mid_classifier, train_data_mid)}\")\n",
    "print(f\"mid_classifier test accuracy: {accuracy(mid_classifier, test_data_mid)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sub-classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Subclassifier(nn.Module):\n",
    "    def __init__(self, output_size, input_size=384, hidden_size1=1000, hidden_size2=1000):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(input_size, hidden_size1)\n",
    "        self.linear2 = nn.Linear(hidden_size1, hidden_size2)\n",
    "        self.linear3 = nn.Linear(hidden_size2 + 1, output_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        k=torch.argmax(top_classifier.forward(x),dim=1)\n",
    "        x = self.linear1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.relu(x)\n",
    "        x = torch.cat((x,torch.unsqueeze(k,1)),dim=1)\n",
    "        x = self.linear3(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "    \n",
    "    def train(self, train_loader, criterion, optimizer, num_epochs):\n",
    "        for epoch in range(num_epochs):\n",
    "            for inputs, labels in train_loader:\n",
    "                # Clear the gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Compute the predicted outputs\n",
    "                outputs = self(inputs)\n",
    "\n",
    "                # Compute the loss\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                # Backpropagate the gradients\n",
    "                loss.backward()\n",
    "                \n",
    "                # Update the model parameters\n",
    "                optimizer.step()\n",
    "            if epoch % 100 == 0:\n",
    "                print(f\"Loss: {loss}, epoch: {epoch}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 4.664001941680908, epoch: 0\n",
      "Loss: 4.671936988830566, epoch: 100\n",
      "Loss: 4.545565605163574, epoch: 200\n",
      "Loss: 4.540782451629639, epoch: 300\n",
      "Loss: 4.52800989151001, epoch: 400\n",
      "Loss: 4.457130432128906, epoch: 500\n",
      "Loss: 4.67572546005249, epoch: 600\n",
      "Loss: 4.43361759185791, epoch: 700\n",
      "Loss: 4.676501750946045, epoch: 800\n",
      "Loss: 4.671538352966309, epoch: 900\n"
     ]
    }
   ],
   "source": [
    "mid_classifier2 = Subclassifier(output_size=len(mid_level))\n",
    "\n",
    "mid_classifier2.train(\n",
    "    train_loader=DataLoader(train_data_mid, shuffle=True, batch_size=16),\n",
    "    criterion=nn.CrossEntropyLoss(),\n",
    "    optimizer=torch.optim.SGD(mid_classifier2.parameters(), lr=0.02),\n",
    "    num_epochs=1000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mid_classifier train accuracy: 0.098\n",
      "mid_classifier test accuracy: 0.09732360097323602\n",
      "mid_classifier2 train accuracy: 0.187\n",
      "mid_classifier2 test accuracy: 0.1362530413625304\n"
     ]
    }
   ],
   "source": [
    "print(f\"mid_classifier train accuracy: {accuracy(mid_classifier, train_data_mid)}\")\n",
    "print(f\"mid_classifier test accuracy: {accuracy(mid_classifier, test_data_mid)}\")\n",
    "print(f\"mid_classifier2 train accuracy: {accuracy(mid_classifier2, train_data_mid)}\")\n",
    "print(f\"mid_classifier2 test accuracy: {accuracy(mid_classifier2, test_data_mid)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d88c74c87f51e8694957b28af56f8f9d57fbf8ebab5eb40d5fd4f68319a3140f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
