{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, InputExample, losses\n",
    "from transformers import AutoModel\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "patent_data = pd.read_csv('df_claim_cpc_1400.csv',encoding='unicode_escape')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sort the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-processing to get the possible labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_level= {}\n",
    "mid_level = {}\n",
    "group_level = {}\n",
    "\n",
    "i=0\n",
    "for label in patent_data.iterrows():\n",
    "    tl = label[1]['group_id'][2]\n",
    "    ml = label[1]['group_id'][2:5]\n",
    "    gl = label[1]['group_id'][2:6]\n",
    "    sentence = label[1]['text'].encode().decode(\"utf-8\")\n",
    "    \n",
    "    if tl in top_level:\n",
    "        top_level[tl].append(sentence)\n",
    "    else:\n",
    "        top_level[tl] = [sentence]\n",
    "    \n",
    "    if ml in mid_level:\n",
    "        mid_level[ml].append(sentence)\n",
    "    else:\n",
    "        mid_level[ml] = [sentence]\n",
    "\n",
    "    if gl in group_level:\n",
    "        group_level[gl].append(sentence)\n",
    "    else:\n",
    "        group_level[gl] = [sentence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_labels(classes):\n",
    "    labels = {}\n",
    "    for i,l in enumerate(classes):\n",
    "        labels[l] = float(i)\n",
    "    return labels\n",
    "\n",
    "top_labels = gen_labels(top_level)\n",
    "mid_labels = gen_labels(mid_level)\n",
    "group_labels = gen_labels(group_level)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tune SBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [27], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m sbert_model \u001b[39m=\u001b[39m SentenceTransformer(\u001b[39m'\u001b[39;49m\u001b[39mall-MiniLM-L6-v2\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m      2\u001b[0m sentences \u001b[39m=\u001b[39m patent_data[\u001b[39m'\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m      3\u001b[0m groups \u001b[39m=\u001b[39m patent_data[\u001b[39m'\u001b[39m\u001b[39mgroup_id\u001b[39m\u001b[39m'\u001b[39m]\n",
      "File \u001b[0;32m~/cs240/env/lib/python3.8/site-packages/sentence_transformers/SentenceTransformer.py:95\u001b[0m, in \u001b[0;36mSentenceTransformer.__init__\u001b[0;34m(self, model_name_or_path, modules, device, cache_folder, use_auth_token)\u001b[0m\n\u001b[1;32m     87\u001b[0m         snapshot_download(model_name_or_path,\n\u001b[1;32m     88\u001b[0m                             cache_dir\u001b[39m=\u001b[39mcache_folder,\n\u001b[1;32m     89\u001b[0m                             library_name\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39msentence-transformers\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m     90\u001b[0m                             library_version\u001b[39m=\u001b[39m__version__,\n\u001b[1;32m     91\u001b[0m                             ignore_files\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mflax_model.msgpack\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mrust_model.ot\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mtf_model.h5\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[1;32m     92\u001b[0m                             use_auth_token\u001b[39m=\u001b[39muse_auth_token)\n\u001b[1;32m     94\u001b[0m \u001b[39mif\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mexists(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(model_path, \u001b[39m'\u001b[39m\u001b[39mmodules.json\u001b[39m\u001b[39m'\u001b[39m)):    \u001b[39m#Load as SentenceTransformer model\u001b[39;00m\n\u001b[0;32m---> 95\u001b[0m     modules \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_load_sbert_model(model_path)\n\u001b[1;32m     96\u001b[0m \u001b[39melse\u001b[39;00m:   \u001b[39m#Load with AutoModel\u001b[39;00m\n\u001b[1;32m     97\u001b[0m     modules \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_load_auto_model(model_path)\n",
      "File \u001b[0;32m~/cs240/env/lib/python3.8/site-packages/sentence_transformers/SentenceTransformer.py:840\u001b[0m, in \u001b[0;36mSentenceTransformer._load_sbert_model\u001b[0;34m(self, model_path)\u001b[0m\n\u001b[1;32m    838\u001b[0m \u001b[39mfor\u001b[39;00m module_config \u001b[39min\u001b[39;00m modules_config:\n\u001b[1;32m    839\u001b[0m     module_class \u001b[39m=\u001b[39m import_from_string(module_config[\u001b[39m'\u001b[39m\u001b[39mtype\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m--> 840\u001b[0m     module \u001b[39m=\u001b[39m module_class\u001b[39m.\u001b[39;49mload(os\u001b[39m.\u001b[39;49mpath\u001b[39m.\u001b[39;49mjoin(model_path, module_config[\u001b[39m'\u001b[39;49m\u001b[39mpath\u001b[39;49m\u001b[39m'\u001b[39;49m]))\n\u001b[1;32m    841\u001b[0m     modules[module_config[\u001b[39m'\u001b[39m\u001b[39mname\u001b[39m\u001b[39m'\u001b[39m]] \u001b[39m=\u001b[39m module\n\u001b[1;32m    843\u001b[0m \u001b[39mreturn\u001b[39;00m modules\n",
      "File \u001b[0;32m~/cs240/env/lib/python3.8/site-packages/sentence_transformers/models/Transformer.py:137\u001b[0m, in \u001b[0;36mTransformer.load\u001b[0;34m(input_path)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(sbert_config_path) \u001b[39mas\u001b[39;00m fIn:\n\u001b[1;32m    136\u001b[0m     config \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39mload(fIn)\n\u001b[0;32m--> 137\u001b[0m \u001b[39mreturn\u001b[39;00m Transformer(model_name_or_path\u001b[39m=\u001b[39;49minput_path, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mconfig)\n",
      "File \u001b[0;32m~/cs240/env/lib/python3.8/site-packages/sentence_transformers/models/Transformer.py:29\u001b[0m, in \u001b[0;36mTransformer.__init__\u001b[0;34m(self, model_name_or_path, max_seq_length, model_args, cache_dir, tokenizer_args, do_lower_case, tokenizer_name_or_path)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdo_lower_case \u001b[39m=\u001b[39m do_lower_case\n\u001b[1;32m     28\u001b[0m config \u001b[39m=\u001b[39m AutoConfig\u001b[39m.\u001b[39mfrom_pretrained(model_name_or_path, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_args, cache_dir\u001b[39m=\u001b[39mcache_dir)\n\u001b[0;32m---> 29\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_load_model(model_name_or_path, config, cache_dir)\n\u001b[1;32m     31\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenizer \u001b[39m=\u001b[39m AutoTokenizer\u001b[39m.\u001b[39mfrom_pretrained(tokenizer_name_or_path \u001b[39mif\u001b[39;00m tokenizer_name_or_path \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m model_name_or_path, cache_dir\u001b[39m=\u001b[39mcache_dir, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mtokenizer_args)\n\u001b[1;32m     33\u001b[0m \u001b[39m#No max_seq_length set. Try to infer from model\u001b[39;00m\n",
      "File \u001b[0;32m~/cs240/env/lib/python3.8/site-packages/sentence_transformers/models/Transformer.py:49\u001b[0m, in \u001b[0;36mTransformer._load_model\u001b[0;34m(self, model_name_or_path, config, cache_dir)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_load_t5_model(model_name_or_path, config, cache_dir)\n\u001b[1;32m     48\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 49\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_model \u001b[39m=\u001b[39m AutoModel\u001b[39m.\u001b[39;49mfrom_pretrained(model_name_or_path, config\u001b[39m=\u001b[39;49mconfig, cache_dir\u001b[39m=\u001b[39;49mcache_dir)\n",
      "File \u001b[0;32m~/cs240/env/lib/python3.8/site-packages/transformers/models/auto/auto_factory.py:463\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mtype\u001b[39m(config) \u001b[39min\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping\u001b[39m.\u001b[39mkeys():\n\u001b[1;32m    462\u001b[0m     model_class \u001b[39m=\u001b[39m _get_model_class(config, \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 463\u001b[0m     \u001b[39mreturn\u001b[39;00m model_class\u001b[39m.\u001b[39;49mfrom_pretrained(\n\u001b[1;32m    464\u001b[0m         pretrained_model_name_or_path, \u001b[39m*\u001b[39;49mmodel_args, config\u001b[39m=\u001b[39;49mconfig, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mhub_kwargs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    465\u001b[0m     )\n\u001b[1;32m    466\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    467\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnrecognized configuration class \u001b[39m\u001b[39m{\u001b[39;00mconfig\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m for this kind of AutoModel: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    468\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mModel type should be one of \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(c\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m \u001b[39mfor\u001b[39;00m c \u001b[39min\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping\u001b[39m.\u001b[39mkeys())\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    469\u001b[0m )\n",
      "File \u001b[0;32m~/cs240/env/lib/python3.8/site-packages/transformers/modeling_utils.py:2184\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   2181\u001b[0m \u001b[39mif\u001b[39;00m from_pt:\n\u001b[1;32m   2182\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_sharded \u001b[39mand\u001b[39;00m state_dict \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   2183\u001b[0m         \u001b[39m# Time to load the checkpoint\u001b[39;00m\n\u001b[0;32m-> 2184\u001b[0m         state_dict \u001b[39m=\u001b[39m load_state_dict(resolved_archive_file)\n\u001b[1;32m   2186\u001b[0m     \u001b[39m# set dtype to instantiate the model under:\u001b[39;00m\n\u001b[1;32m   2187\u001b[0m     \u001b[39m# 1. If torch_dtype is not None, we use that dtype\u001b[39;00m\n\u001b[1;32m   2188\u001b[0m     \u001b[39m# 2. If torch_dtype is \"auto\", we auto-detect dtype from the loaded state_dict, by checking its first\u001b[39;00m\n\u001b[1;32m   2189\u001b[0m     \u001b[39m#    weights entry that is of a floating type - we assume all floating dtype weights are of the same dtype\u001b[39;00m\n\u001b[1;32m   2190\u001b[0m     \u001b[39m# we also may have config.torch_dtype available, but we won't rely on it till v5\u001b[39;00m\n\u001b[1;32m   2191\u001b[0m     dtype_orig \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/cs240/env/lib/python3.8/site-packages/transformers/modeling_utils.py:399\u001b[0m, in \u001b[0;36mload_state_dict\u001b[0;34m(checkpoint_file)\u001b[0m\n\u001b[1;32m    397\u001b[0m     \u001b[39mreturn\u001b[39;00m safe_load_file(checkpoint_file)\n\u001b[1;32m    398\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 399\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mload(checkpoint_file, map_location\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcpu\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m    400\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    401\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/cs240/env/lib/python3.8/site-packages/torch/serialization.py:712\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    710\u001b[0m             opened_file\u001b[39m.\u001b[39mseek(orig_position)\n\u001b[1;32m    711\u001b[0m             \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mjit\u001b[39m.\u001b[39mload(opened_file)\n\u001b[0;32m--> 712\u001b[0m         \u001b[39mreturn\u001b[39;00m _load(opened_zipfile, map_location, pickle_module, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mpickle_load_args)\n\u001b[1;32m    713\u001b[0m \u001b[39mreturn\u001b[39;00m _legacy_load(opened_file, map_location, pickle_module, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpickle_load_args)\n",
      "File \u001b[0;32m~/cs240/env/lib/python3.8/site-packages/torch/serialization.py:1049\u001b[0m, in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1047\u001b[0m unpickler \u001b[39m=\u001b[39m UnpicklerWrapper(data_file, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpickle_load_args)\n\u001b[1;32m   1048\u001b[0m unpickler\u001b[39m.\u001b[39mpersistent_load \u001b[39m=\u001b[39m persistent_load\n\u001b[0;32m-> 1049\u001b[0m result \u001b[39m=\u001b[39m unpickler\u001b[39m.\u001b[39;49mload()\n\u001b[1;32m   1051\u001b[0m torch\u001b[39m.\u001b[39m_utils\u001b[39m.\u001b[39m_validate_loaded_sparse_tensors()\n\u001b[1;32m   1053\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/cs240/env/lib/python3.8/site-packages/torch/serialization.py:1019\u001b[0m, in \u001b[0;36m_load.<locals>.persistent_load\u001b[0;34m(saved_id)\u001b[0m\n\u001b[1;32m   1017\u001b[0m \u001b[39mif\u001b[39;00m key \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m loaded_storages:\n\u001b[1;32m   1018\u001b[0m     nbytes \u001b[39m=\u001b[39m numel \u001b[39m*\u001b[39m torch\u001b[39m.\u001b[39m_utils\u001b[39m.\u001b[39m_element_size(dtype)\n\u001b[0;32m-> 1019\u001b[0m     load_tensor(dtype, nbytes, key, _maybe_decode_ascii(location))\n\u001b[1;32m   1021\u001b[0m \u001b[39mreturn\u001b[39;00m loaded_storages[key]\n",
      "File \u001b[0;32m~/cs240/env/lib/python3.8/site-packages/torch/serialization.py:997\u001b[0m, in \u001b[0;36m_load.<locals>.load_tensor\u001b[0;34m(dtype, numel, key, location)\u001b[0m\n\u001b[1;32m    994\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload_tensor\u001b[39m(dtype, numel, key, location):\n\u001b[1;32m    995\u001b[0m     name \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mdata/\u001b[39m\u001b[39m{\u001b[39;00mkey\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\n\u001b[0;32m--> 997\u001b[0m     storage \u001b[39m=\u001b[39m zip_file\u001b[39m.\u001b[39;49mget_storage_from_record(name, numel, torch\u001b[39m.\u001b[39;49m_UntypedStorage)\u001b[39m.\u001b[39mstorage()\u001b[39m.\u001b[39m_untyped()\n\u001b[1;32m    998\u001b[0m     \u001b[39m# TODO: Once we decide to break serialization FC, we can\u001b[39;00m\n\u001b[1;32m    999\u001b[0m     \u001b[39m# stop wrapping with _TypedStorage\u001b[39;00m\n\u001b[1;32m   1000\u001b[0m     loaded_storages[key] \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mstorage\u001b[39m.\u001b[39m_TypedStorage(\n\u001b[1;32m   1001\u001b[0m         wrap_storage\u001b[39m=\u001b[39mrestore_location(storage, location),\n\u001b[1;32m   1002\u001b[0m         dtype\u001b[39m=\u001b[39mdtype)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "sbert_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "sentences = patent_data['text']\n",
    "groups = patent_data['group_id']\n",
    "\n",
    "train_examples = []\n",
    "\n",
    "for t in group_level: \n",
    "    for k in group_level[t]:\n",
    "        train_examples.append(InputExample(texts=k, label=group_labels[t])) # can fine-tune on other label types\n",
    "\n",
    "#Define train dataset, the dataloader and the train loss\n",
    "train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=1)\n",
    "train_loss = losses.CosineSimilarityLoss(sbert_model)\n",
    "\n",
    "do_train = False\n",
    "\n",
    "if do_train:\n",
    "    sbert_model.fit(train_objectives=[(train_dataloader, train_loss)], epochs=2)\n",
    "    sbert_model.save(\"sbert\")\n",
    "else:\n",
    "    sbert_model = SentenceTransformer('sbert')\n",
    "\n",
    "embeddings = sbert_model.encode(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define generic softmax classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, output_size, input_size=384, hidden_size1=1000, hidden_size2=1000):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(input_size, hidden_size1)\n",
    "        self.linear2 = nn.Linear(hidden_size1, hidden_size2)\n",
    "        self.linear3 = nn.Linear(hidden_size2, output_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.linear3(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "    \n",
    "    def train(self, train_loader, criterion, optimizer, num_epochs):\n",
    "        for epoch in range(num_epochs):\n",
    "            for inputs, labels in train_loader:\n",
    "                # Clear the gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Compute the predicted outputs\n",
    "                outputs = self(inputs)\n",
    "\n",
    "                # Compute the loss\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                # Backpropagate the gradients\n",
    "                loss.backward()\n",
    "                \n",
    "                # Update the model parameters\n",
    "                optimizer.step()\n",
    "            if epoch % 10 == 0:\n",
    "                print(f\"Loss: {loss}, epoch: {epoch}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test-train split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_640/2657597474.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  embeddings = torch.tensor(embeddings)\n"
     ]
    }
   ],
   "source": [
    "train_data_top = []\n",
    "test_data_top = []\n",
    "embeddings = torch.tensor(embeddings)\n",
    "for i in range(len(embeddings)):\n",
    "    if i < 1000:\n",
    "        train_data_top.append([embeddings[i], int(top_labels[patent_data['group_id'][i][2]])])\n",
    "    else:\n",
    "        test_data_top.append([embeddings[i], int(top_labels[patent_data['group_id'][i][2]])])\n",
    "\n",
    "train_data_mid = []\n",
    "test_data_mid = []\n",
    "\n",
    "for i in range(len(embeddings)):\n",
    "    if i < 1000:\n",
    "        train_data_mid.append([embeddings[i], int(mid_labels[patent_data['group_id'][i][2:5]])])\n",
    "    else:\n",
    "        test_data_mid.append([embeddings[i], int(mid_labels[patent_data['group_id'][i][2:5]])])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.1885478496551514, epoch: 0\n",
      "Loss: 2.2044105529785156, epoch: 10\n",
      "Loss: 2.1265623569488525, epoch: 20\n",
      "Loss: 1.9089206457138062, epoch: 30\n",
      "Loss: 1.8980792760849, epoch: 40\n",
      "Loss: 1.7630351781845093, epoch: 50\n",
      "Loss: 1.661988615989685, epoch: 60\n",
      "Loss: 1.6849559545516968, epoch: 70\n",
      "Loss: 1.872053861618042, epoch: 80\n",
      "Loss: 1.7781860828399658, epoch: 90\n"
     ]
    }
   ],
   "source": [
    "top_classifier = Classifier(output_size=len(top_level))\n",
    "\n",
    "top_classifier.train(\n",
    "    train_loader=DataLoader(train_data_top, shuffle=True, batch_size=10),\n",
    "    criterion=nn.CrossEntropyLoss(),\n",
    "    optimizer=torch.optim.SGD(top_classifier.parameters(), lr=0.01, momentum=0.9),\n",
    "    num_epochs=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 4.6634840965271, epoch: 0\n",
      "Loss: 4.663344860076904, epoch: 10\n",
      "Loss: 4.663525581359863, epoch: 20\n",
      "Loss: 4.663418292999268, epoch: 30\n",
      "Loss: 4.66337251663208, epoch: 40\n",
      "Loss: 4.663315773010254, epoch: 50\n",
      "Loss: 4.663266181945801, epoch: 60\n",
      "Loss: 4.663301467895508, epoch: 70\n",
      "Loss: 4.663339614868164, epoch: 80\n",
      "Loss: 4.663333415985107, epoch: 90\n"
     ]
    }
   ],
   "source": [
    "mid_classifier = Classifier(output_size=len(mid_level))\n",
    "\n",
    "mid_classifier.train(\n",
    "    train_loader=DataLoader(train_data_mid, shuffle=True, batch_size=32),\n",
    "    criterion=nn.CrossEntropyLoss(),\n",
    "    optimizer=torch.optim.SGD(mid_classifier.parameters(), lr=0.02),\n",
    "    num_epochs=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.635\n",
      "Test accuracy: 0.41849148418491483\n"
     ]
    }
   ],
   "source": [
    "accurate = 0\n",
    "\n",
    "for e,l in train_data_top:\n",
    "    sm = top_classifier.forward(torch.unsqueeze(e,0))\n",
    "    if int(torch.argmax(sm)) == l:\n",
    "        accurate += 1\n",
    "print(f\"Train accuracy: {accurate/len(train_data_top)}\")\n",
    "accurate = 0\n",
    "for e,l in test_data_top:\n",
    "    sm = top_classifier.forward(torch.unsqueeze(e,0))\n",
    "    if int(torch.argmax(sm)) == l:\n",
    "        accurate += 1\n",
    "print(f\"Test accuracy: {accurate/len(test_data_top)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.41362530413625304"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accurate = 0\n",
    "\n",
    "for e,l in test_data_mid:\n",
    "    sm = top_classifier.forward(torch.unsqueeze(e,0))\n",
    "    if int(torch.argmax(sm)) == l:\n",
    "        accurate += 1\n",
    "accurate/len(test_data_mid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sub-classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Subclassifier(nn.Module):\n",
    "    def __init__(self, output_size, input_size=384, hidden_size1=1000, hidden_size2=1000):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(input_size, hidden_size1)\n",
    "        self.linear2 = nn.Linear(hidden_size1, hidden_size2)\n",
    "        self.linear3 = nn.Linear(hidden_size2 + 1, output_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        k=torch.argmax(top_classifier.forward(x),dim=1)\n",
    "        x = self.linear1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.relu(x)\n",
    "        x = torch.cat((x,torch.unsqueeze(k,1)),dim=1)\n",
    "        x = self.linear3(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "    \n",
    "    def train(self, train_loader, criterion, optimizer, num_epochs):\n",
    "        for epoch in range(num_epochs):\n",
    "            for inputs, labels in train_loader:\n",
    "                # Clear the gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Compute the predicted outputs\n",
    "                outputs = self(inputs)\n",
    "\n",
    "                # Compute the loss\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                # Backpropagate the gradients\n",
    "                loss.backward()\n",
    "                \n",
    "                # Update the model parameters\n",
    "                optimizer.step()\n",
    "            if epoch % 10 == 0:\n",
    "                print(f\"Loss: {loss}, epoch: {epoch}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 4.663654327392578, epoch: 0\n",
      "Loss: 4.663252353668213, epoch: 10\n",
      "Loss: 4.661838054656982, epoch: 20\n",
      "Loss: 4.663501739501953, epoch: 30\n",
      "Loss: 4.653690338134766, epoch: 40\n",
      "Loss: 4.66337776184082, epoch: 50\n",
      "Loss: 4.437601566314697, epoch: 60\n",
      "Loss: 4.667328834533691, epoch: 70\n",
      "Loss: 4.424098491668701, epoch: 80\n",
      "Loss: 4.670032501220703, epoch: 90\n",
      "Loss: 4.667972564697266, epoch: 100\n",
      "Loss: 4.546344757080078, epoch: 110\n",
      "Loss: 4.544979572296143, epoch: 120\n",
      "Loss: 4.551736831665039, epoch: 130\n",
      "Loss: 4.546972751617432, epoch: 140\n",
      "Loss: 4.666375637054443, epoch: 150\n",
      "Loss: 4.669826507568359, epoch: 160\n",
      "Loss: 4.549212455749512, epoch: 170\n",
      "Loss: 4.544863700866699, epoch: 180\n",
      "Loss: 4.547497272491455, epoch: 190\n",
      "Loss: 4.546455383300781, epoch: 200\n",
      "Loss: 4.666857719421387, epoch: 210\n",
      "Loss: 4.543020248413086, epoch: 220\n",
      "Loss: 4.542840480804443, epoch: 230\n",
      "Loss: 4.424198627471924, epoch: 240\n",
      "Loss: 4.670831680297852, epoch: 250\n",
      "Loss: 4.5482072830200195, epoch: 260\n",
      "Loss: 4.421463489532471, epoch: 270\n",
      "Loss: 4.5474534034729, epoch: 280\n",
      "Loss: 4.66986608505249, epoch: 290\n",
      "Loss: 4.424498558044434, epoch: 300\n",
      "Loss: 4.423398971557617, epoch: 310\n",
      "Loss: 4.669524192810059, epoch: 320\n",
      "Loss: 4.543919086456299, epoch: 330\n",
      "Loss: 4.301192283630371, epoch: 340\n",
      "Loss: 4.544277667999268, epoch: 350\n",
      "Loss: 4.668872833251953, epoch: 360\n",
      "Loss: 4.6651740074157715, epoch: 370\n",
      "Loss: 4.420782089233398, epoch: 380\n",
      "Loss: 4.665737628936768, epoch: 390\n",
      "Loss: 4.547944068908691, epoch: 400\n",
      "Loss: 4.422309875488281, epoch: 410\n",
      "Loss: 4.66929817199707, epoch: 420\n",
      "Loss: 4.545079231262207, epoch: 430\n",
      "Loss: 4.668278217315674, epoch: 440\n",
      "Loss: 4.542729377746582, epoch: 450\n",
      "Loss: 4.358098983764648, epoch: 460\n",
      "Loss: 4.548554420471191, epoch: 470\n",
      "Loss: 4.479461669921875, epoch: 480\n",
      "Loss: 4.666418075561523, epoch: 490\n",
      "Loss: 4.664607048034668, epoch: 500\n",
      "Loss: 4.542717456817627, epoch: 510\n",
      "Loss: 4.66731595993042, epoch: 520\n",
      "Loss: 4.426905155181885, epoch: 530\n",
      "Loss: 4.667649269104004, epoch: 540\n",
      "Loss: 4.67056941986084, epoch: 550\n",
      "Loss: 4.666402816772461, epoch: 560\n",
      "Loss: 4.6663360595703125, epoch: 570\n",
      "Loss: 4.542256832122803, epoch: 580\n",
      "Loss: 4.424570083618164, epoch: 590\n",
      "Loss: 4.540635585784912, epoch: 600\n",
      "Loss: 4.664783000946045, epoch: 610\n",
      "Loss: 4.661314010620117, epoch: 620\n",
      "Loss: 4.417063236236572, epoch: 630\n",
      "Loss: 4.544569492340088, epoch: 640\n",
      "Loss: 4.542280197143555, epoch: 650\n",
      "Loss: 4.545929908752441, epoch: 660\n",
      "Loss: 4.423295497894287, epoch: 670\n",
      "Loss: 4.546729564666748, epoch: 680\n",
      "Loss: 4.670455455780029, epoch: 690\n",
      "Loss: 4.6667046546936035, epoch: 700\n",
      "Loss: 4.666878700256348, epoch: 710\n",
      "Loss: 4.6627278327941895, epoch: 720\n",
      "Loss: 4.663212776184082, epoch: 730\n",
      "Loss: 4.534642219543457, epoch: 740\n",
      "Loss: 4.538145542144775, epoch: 750\n",
      "Loss: 4.540219783782959, epoch: 760\n",
      "Loss: 4.653659820556641, epoch: 770\n",
      "Loss: 4.393235683441162, epoch: 780\n",
      "Loss: 4.544699668884277, epoch: 790\n",
      "Loss: 4.650283336639404, epoch: 800\n",
      "Loss: 4.530540466308594, epoch: 810\n",
      "Loss: 4.3104329109191895, epoch: 820\n",
      "Loss: 4.671056747436523, epoch: 830\n",
      "Loss: 4.667240142822266, epoch: 840\n",
      "Loss: 4.431314468383789, epoch: 850\n",
      "Loss: 4.547268867492676, epoch: 860\n",
      "Loss: 4.485191822052002, epoch: 870\n",
      "Loss: 4.659928798675537, epoch: 880\n",
      "Loss: 4.470909118652344, epoch: 890\n",
      "Loss: 4.621345520019531, epoch: 900\n",
      "Loss: 4.440282821655273, epoch: 910\n",
      "Loss: 4.609962463378906, epoch: 920\n",
      "Loss: 4.672950744628906, epoch: 930\n",
      "Loss: 4.672027111053467, epoch: 940\n",
      "Loss: 4.43270206451416, epoch: 950\n",
      "Loss: 4.45788049697876, epoch: 960\n",
      "Loss: 4.664695739746094, epoch: 970\n",
      "Loss: 4.666518211364746, epoch: 980\n",
      "Loss: 4.5487060546875, epoch: 990\n"
     ]
    }
   ],
   "source": [
    "mid_classifier2 = Subclassifier(output_size=len(mid_level))\n",
    "\n",
    "mid_classifier2.train(\n",
    "    train_loader=DataLoader(train_data_mid, shuffle=True, batch_size=32),\n",
    "    criterion=nn.CrossEntropyLoss(),\n",
    "    optimizer=torch.optim.SGD(mid_classifier2.parameters(), lr=0.02),\n",
    "    num_epochs=1000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09732360097323602 0.14841849148418493\n"
     ]
    }
   ],
   "source": [
    "accurate1 = 0\n",
    "accurate2 = 0\n",
    "\n",
    "for e,l in test_data_mid:\n",
    "    sm1 = mid_classifier.forward(torch.unsqueeze(e,0))\n",
    "    sm2 = mid_classifier2.forward(torch.unsqueeze(e,0))\n",
    "    if int(torch.argmax(sm1)) == l:\n",
    "        accurate1 += 1\n",
    "    if int(torch.argmax(sm2)) == l:\n",
    "        accurate2 += 1\n",
    "       \n",
    "print(accurate1/len(test_data_mid),accurate2/len(test_data_mid))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d88c74c87f51e8694957b28af56f8f9d57fbf8ebab5eb40d5fd4f68319a3140f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
